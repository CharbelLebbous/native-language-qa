# âœ… Import core modules:
# - streamlit: web UI framework
# - os, pickle: standard Python libs (pickle is unused here)
# - loader, embedder, qa: your custom pipeline modules
# - FAISS: vector store class (imported but not directly used here)
import streamlit as st
import os
import pickle  # Note: not used in current code
from loader import load_documents
from embedder import EmbeddingIndexer
from qa import answer_question
from langchain_community.vectorstores import FAISS

# ğŸš€ Set the page title
st.title("ğŸ“š Native Language QA on Documents")

# âœ… Create a 3-column layout to place the exit button on the right
col1, col2, col3 = st.columns([4, 1, 1])
with col3:
    if st.button("âŒ Exit App"):
        st.warning("App terminated.")
        st.stop()  # Stops the Streamlit script immediately

# ğŸ“‚ Text input for the folder path with a default suggestion
folder_path = st.text_input("ğŸ“ Enter path to document folder:", "./data")

# ğŸ“‚ Load Folder button:
# 1ï¸âƒ£ Load all files from folder if not loaded yet.
# 2ï¸âƒ£ If path changed, clear old index and reload.
if st.button("ğŸ“‚ Load Folder"):
    if (
        "indexer" not in st.session_state
        or folder_path != st.session_state.get("loaded_path", "")
    ):
        # Remember the folder path to detect changes later
        st.session_state.loaded_path = folder_path
        st.session_state.indexer = None
        st.session_state.docs = None

        with st.spinner("ğŸ”„ Loading & building index from folder..."):
            # Call your custom loader to read and chunk all valid docs
            st.session_state.docs = load_documents(folder_path)

            if not st.session_state.docs:
                st.error("âŒ No valid documents found in the selected folder.")
            else:
                # Initialize the embedder and build the FAISS index
                st.session_state.indexer = EmbeddingIndexer()
                st.session_state.indexer.build_index(st.session_state.docs)

# âœ… If index is ready, show question input and answer output
if "indexer" in st.session_state and st.session_state.indexer:
    query = st.text_input("ğŸ” Ask a question:")

    if query:
        # ğŸ” Search the vector store for top-k similar chunks
        results = st.session_state.indexer.search(query)

        # ğŸ—‚ï¸ Build metadata for source display (file name, page, path)
        sources = []
        for doc in results:
            file_name = doc.metadata.get("file_name", "Unknown File")
            page = doc.metadata.get("page", "N/A")
            source_path = doc.metadata.get("source", "Unknown Path")
            source_str = f"ğŸ“„ **{file_name}** â€” Page: {page}  \nğŸ“ Path: `{source_path}`"
            sources.append(source_str)

        # âœ… Remove duplicates but keep order
        unique_sources = list(dict.fromkeys(sources))

        # ğŸ“ Show the top retrieved contexts (text chunks)
        st.write("ğŸ” **Top Contexts Retrieved:**")
        for i, doc in enumerate(results):
            st.text_area(
                f"Chunk {i+1} (From: {sources[i]})",
                doc.page_content[:500],
                height=150
            )

        # ğŸ§  Run LLM to generate the final answer
        with st.spinner("ğŸ¤– Generating answer..."):
            answer = answer_question(query, results)

        # âœ… Show final answer + sources
        st.markdown(f"**ğŸ’¬ Answer:** {answer}")
        st.markdown("**ğŸ“„ Sources Used:**")
        for src in unique_sources:
            st.markdown(src)

else:
    # ğŸ“Œ Reminder for the user to load a folder first
    st.info("ğŸ‘† Select a folder and press **Load Folder** to start.")
